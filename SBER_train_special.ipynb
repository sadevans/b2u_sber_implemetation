{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "from sklearn.feature_extraction import image\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from arch_unet import UNet\n",
    "import utils as util\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from Sber_utils import Masker, save_network, load_network, save_state, resume_state, checkpoint, generate_mask, interpolate_mask, depth_to_space, get_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--noisetype\", type=str, default=\"gauss25\", choices=['gauss25', 'gauss5_50', 'poisson30', 'poisson5_50'])\n",
    "parser.add_argument('--resume', type=str)\n",
    "parser.add_argument('--checkpoint', type=str)\n",
    "parser.add_argument('--train_dir', type=str, default='./dataset/small/train')\n",
    "parser.add_argument('--test_dir', type=str, default='./dataset/small/validation')\n",
    "parser.add_argument('--save_test_path', type=str, default='./test')\n",
    "parser.add_argument('--save_model_path', type=str, default='../experiments/small')\n",
    "parser.add_argument('--log_name', type=str, default='xxx_b2u_unet_fmdd_112rf20')\n",
    "parser.add_argument('--gpu_devices', default='0', type=str)\n",
    "parser.add_argument('--parallel', action='store_true')\n",
    "parser.add_argument('--n_feature', type=int, default=48)\n",
    "parser.add_argument('--n_channel', type=int, default=1)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--w_decay', type=float, default=1e-8)\n",
    "parser.add_argument('--gamma', type=float, default=0.5)\n",
    "parser.add_argument('--n_epoch', type=int, default=100)\n",
    "parser.add_argument('--n_snapshot', type=int, default=1)\n",
    "parser.add_argument('--batchsize', type=int, default=4)\n",
    "parser.add_argument('--patchsize', type=int, default=128)\n",
    "parser.add_argument(\"--Lambda1\", type=float, default=1.0)\n",
    "parser.add_argument(\"--Lambda2\", type=float, default=2.0)\n",
    "parser.add_argument(\"--increase_ratio\", type=float, default=20.0)\n",
    "parser.add_argument('--test_dataset_name', type=str)\n",
    "parser.add_argument('--train_dataset_name', type=str)\n",
    "parser.add_argument('--repeat_times', type=int, default=3)\n",
    "\n",
    "\n",
    "\n",
    "opt = parser.parse_args([\n",
    "                        '--train_dir', './data/train', \n",
    "                        '--test_dir', './data/test',\n",
    "                        '--save_test_path', './test',\n",
    "                        '--save_model_path', './experiments/my_models', \n",
    "                        '--log_name', 'b2u_g146_first',\n",
    "                        '--test_dataset_name', 'validation_g146',\n",
    "                        '--train_dataset_name', 'G146_first',\n",
    "                        '--Lambda1', '1.0',\n",
    "                        '--Lambda2', '2.0',\n",
    "                        '--increase_ratio', '20.0',\n",
    "                        '--n_epoch', '100',\n",
    "                        '--batchsize', '2'\n",
    "                        ])\n",
    "                        \n",
    "systime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "operation_seed_counter = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_devices\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "opt.save_path = os.path.join(opt.save_model_path, opt.log_name)\n",
    "os.makedirs(opt.save_path, exist_ok=True)\n",
    "os.makedirs('./metrics/', exist_ok=True)\n",
    "\n",
    "# imgs_dir = os.path.join()\n",
    "# os.makedirs(os.path.join('./metrics/', opt.train_dataset_name + '/'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validation(imgs_dir, val_dir, max_len_val, patch_size, max_patches, random_state=42):\n",
    "    imgs = sorted(os.listdir('./data/train/G146_first/'))\n",
    "    imgs_patches = []\n",
    "    \n",
    "    for img in imgs:\n",
    "        img_ = cv2.imread('./data/train/G146_first/' + img, 0)\n",
    "        patchesimgs = image.extract_patches_2d(img_, patch_size=patch_size, max_patches=max_patches, random_state=random_state)\n",
    "        \n",
    "        for j in range(patchesimgs.shape[0]):\n",
    "            imgs_patches.append(patchesimgs[j])\n",
    "    ind_val = random.sample(range(len(imgs_patches)), max_len_val)\n",
    "\n",
    "    X_val = [imgs_patches[i] for i in ind_val]\n",
    "\n",
    "    count = 0\n",
    "    for j in range(0, len(X_val)):\n",
    "        cv2.imwrite('./data/test/validation_g146/' + str(count).zfill(4)+'.tif', X_val[j])\n",
    "        count += 1\n",
    "\n",
    "make_validation(os.path.join(opt.train_dir, '/' + opt.train_dataset_name), os.path.join(opt.test_dir, '/' + opt.test_dataset_name), 30, (512, 512), 100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(prediction, target):\n",
    "    C1 = (0.01 * 255)**2\n",
    "    C2 = (0.03 * 255)**2\n",
    "    img1 = prediction.astype(np.float64)\n",
    "    img2 = target.astype(np.float64)\n",
    "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
    "    window = np.outer(kernel, kernel.transpose())\n",
    "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n",
    "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
    "    mu1_sq = mu1**2\n",
    "    mu2_sq = mu2**2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
    "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
    "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) *\n",
    "                (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
    "                                       (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "\n",
    "def calculate_ssim(target, ref):\n",
    "    '''\n",
    "    calculate SSIM\n",
    "    the same outputs as MATLAB's\n",
    "    img1, img2: [0, 255]\n",
    "    '''\n",
    "    img1 = np.array(target, dtype=np.float64)\n",
    "    img2 = np.array(ref, dtype=np.float64)\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    if img1.ndim == 2:\n",
    "        return ssim(img1, img2)\n",
    "    elif img1.ndim == 3:\n",
    "        if img1.shape[2] == 3:\n",
    "            ssims = []\n",
    "            for i in range(3):\n",
    "                ssims.append(ssim(img1[:, :, i], img2[:, :, i]))\n",
    "            return np.array(ssims).mean()\n",
    "        elif img1.shape[2] == 1:\n",
    "            return ssim(np.squeeze(img1), np.squeeze(img2))\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')\n",
    "\n",
    "\n",
    "def calculate_psnr(target, ref, data_range=255.0):\n",
    "    img1 = np.array(target, dtype=np.float32)\n",
    "    img2 = np.array(ref, dtype=np.float32)\n",
    "    diff = img1 - img2\n",
    "    psnr = 10.0 * np.log10(data_range**2 / np.mean(np.square(diff)))\n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_file(metrics, name_metric, name_dataset):\n",
    "    with open(f\"./metrics/{name_metric}_{name_dataset}.txt\", \"w\") as file:\n",
    "        for metric in metrics:\n",
    "            file.write(str(metric) + '\\n')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_median(img, percentile):\n",
    "    tresh = np.percentile(img.ravel(), percentile)\n",
    "    mask = np.zeros_like(img)\n",
    "    mask[img < tresh] = 1\n",
    "    median_img = cv2.medianBlur(img, 7)\n",
    "    img = img*(1 - mask) + median_img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMicroscopyDataset(Dataset):\n",
    "    def __init__(self, img_dir,channels=3, transform=None, apply_median=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.apply_median = apply_median\n",
    "        self.transform = transform\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "        if channels == 1:\n",
    "            self.channels = 0\n",
    "        else:\n",
    "            self.channels = channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = os.path.split(self.img_filenames[idx])[-1].split('.')[0]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.img_filenames[idx]), self.channels)\n",
    "\n",
    "\n",
    "        if self.apply_median:\n",
    "            image = apply_median(image, 10)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMicroscopyDataset(Dataset):\n",
    "    def __init__(self, img_dir,channels=3, transform=None, patch=256, apply_median=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.apply_median = apply_median\n",
    "        self.transform = transform\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "        if channels == 1:\n",
    "            self.channels = 0\n",
    "        else:\n",
    "            self.channels = channels\n",
    "\n",
    "        self.patch = patch\n",
    "        print('fetch {} samples for training'.format(len(self.img_filenames)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # fetch image\n",
    "        name = os.path.split(self.img_filenames[idx])[-1].split('.')[0]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.img_filenames[idx]), self.channels)\n",
    "        # random crop\n",
    "        if len(image.shape) == 2:\n",
    "            H = image.shape[0]\n",
    "            W = image.shape[1]\n",
    "\n",
    "        CSize = self.patch\n",
    "        rnd_h = np.random.randint(0, max(0, H - CSize))\n",
    "        rnd_w = np.random.randint(0, max(0, W - CSize))\n",
    "        image = image[rnd_h : rnd_h + CSize, rnd_w : rnd_w + CSize]\n",
    "\n",
    "\n",
    "        if self.apply_median:\n",
    "            image = apply_median(image, 10)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch 321 samples for training\n",
      "Batchsize=2, number of epoch=100\n",
      "init finish\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset_dir = os.path.join(opt.train_dir, opt.train_dataset_name)\n",
    "train_dataset = TrainMicroscopyDataset(train_dataset_dir,opt.n_channel, transform=train_transforms, apply_median=False)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, num_workers=8, batch_size=opt.batchsize, shuffle=True)\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset_dir = os.path.join(opt.test_dir, opt.test_dataset_name)\n",
    "test_dataset = TestMicroscopyDataset(test_dataset_dir,opt.n_channel, transform=test_transforms, apply_median=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# Masker\n",
    "masker = Masker(width=4, mode='interpolate', mask_type='all')\n",
    "# Network\n",
    "network = UNet(in_channels=opt.n_channel,\n",
    "                out_channels=opt.n_channel,\n",
    "                wf=opt.n_feature)\n",
    "if opt.parallel:\n",
    "    network = torch.nn.DataParallel(network)\n",
    "network = network.cuda()\n",
    "\n",
    "# validation\n",
    "save_test_path = os.path.join(opt.save_test_path, opt.test_dataset_name)\n",
    "os.makedirs(save_test_path, exist_ok=True)\n",
    "\n",
    "validation_path = os.path.join(save_test_path, opt.log_name)\n",
    "os.makedirs(validation_path, exist_ok=True)\n",
    "# np.random.seed(101)\n",
    "\n",
    "# about training scheme\n",
    "num_epoch = opt.n_epoch\n",
    "ratio = num_epoch / 100\n",
    "optimizer = optim.Adam(network.parameters(), lr=opt.lr,\n",
    "                       weight_decay=opt.w_decay)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
    "                                     milestones=[\n",
    "                                         int(20 * ratio) - 1,\n",
    "                                         int(40 * ratio) - 1,\n",
    "                                         int(60 * ratio) - 1,\n",
    "                                         int(80 * ratio) - 1\n",
    "                                     ],\n",
    "                                     gamma=opt.gamma)\n",
    "print(\"Batchsize={}, number of epoch={}\".format(opt.batchsize, opt.n_epoch))\n",
    "\n",
    "# Resume and load pre-trained model\n",
    "epoch_init = 1\n",
    "if opt.resume is not None:\n",
    "    epoch_init, optimizer, scheduler = resume_state(opt.resume, optimizer, scheduler)\n",
    "if opt.checkpoint is not None:\n",
    "    network = load_network(opt.checkpoint, network, strict=True)\n",
    "\n",
    "# temp\n",
    "if opt.checkpoint is not None:\n",
    "    epoch_init = 65\n",
    "    for i in range(1, epoch_init):\n",
    "        scheduler.step()\n",
    "        new_lr = scheduler.get_lr()[0]\n",
    "        logger.info('----------------------------------------------------')\n",
    "        logger.info(\"==> Resuming Training with learning rate:{}\".format(new_lr))\n",
    "        logger.info('----------------------------------------------------')\n",
    "\n",
    "print('init finish')\n",
    "\n",
    "\n",
    "Thread1 = 0.4\n",
    "Thread2 = 1.0\n",
    "Lambda1 = opt.Lambda1\n",
    "Lambda2 = opt.Lambda2\n",
    "increase_ratio = opt.increase_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 1 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:41<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 2 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 3 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 4 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 5 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 6 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 7 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 8 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 9 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 10 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 11 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 12 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 13 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 14 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 15 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 16 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 17 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 18 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 19 = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 20 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 21 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 22 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 23 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 24 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 25 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 26 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 27 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 28 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 29 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 30 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 31 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 32 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 33 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 34 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 35 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 36 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 37 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 38 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 39 = 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 40 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 41 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 42 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 43 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 44 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 45 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 46 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 47 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 48 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 49 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 50 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 51 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 52 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 53 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 54 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 55 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 56 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 57 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 58 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 59 = 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 60 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 61 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 62 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 63 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 64 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 65 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 66 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 67 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 68 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 69 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 70 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 71 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 72 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 73 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 74 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 75 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 76 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 77 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 78 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 79 = 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 80 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 81 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 82 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 83 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 84 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 85 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 86 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 87 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 88 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 89 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 90 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 91 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 92 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 93 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 94 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 95 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 96 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 97 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 98 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 99 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate of Epoch 100 = 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average psnr for val: 16.097894425028173\n",
      "average ssim for val: 0.13509178911220016\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABCaUlEQVR4nO3dd3hUdRr28XvSJgmkECBNEoiIUqWICKKUJVQFARdRUbGsWGBRUVRAIKBI8QVZG5Z9BV2lWUCwIFFABGkBca2ASlOIEYGEEAhD5rx/8GaWIYFMkpkzc8L3c11cy/zmd8555sHN3DnVZhiGIQAAAIsK8ncBAAAAlUGYAQAAlkaYAQAAlkaYAQAAlkaYAQAAlkaYAQAAlkaYAQAAlkaYAQAAlkaYAQAAlkaYAWCqjIwM2Ww2f5dRYZ06dVKnTp38XQaA0xBmAFTY008/rcWLF/u7DADnOcIMgAqrSJh54okndOzYMd8UBOC8RJgBYIqjR49KkkJCQhQeHu7nagBUJYQZoIopPidl+/btuuWWWxQTE6PatWtr7NixMgxDe/fu1XXXXafo6GglJiZq+vTpJdZRWFio8ePH66KLLpLdbldKSooeffRRFRYWuubYbDYdPXpUb7zxhmw2m2w2m26//Xa3Gn744QfdfPPNqlGjhq666iq398701ltvqU2bNoqMjFSNGjXUoUMHLV++3PV+VlaWunfvrlq1aikiIkJpaWm68847z9mLa6+9VhdeeGGp77Vr106tW7d2vZ49e7b+9re/KT4+Xna7XY0bN9asWbPOuX5JmjNnjmw2m3bt2uU2vmrVKtlsNq1atcptfMOGDerRo4diYmIUGRmpjh07au3atW5zjhw5ogcffFD16tWT3W5XfHy8unbtqi1btpRZD3A+CvF3AQB8Y+DAgWrUqJGmTJmijz76SE899ZTi4uL0yiuv6G9/+5umTp2qt99+W4888oguv/xydejQQZLkdDrVp08frVmzRkOGDFGjRo307bff6tlnn9X27dtdh5X+85//6B//+IfatGmjIUOGSJLq16/vVsOAAQPUoEEDPf300zIM46y1TpgwQRkZGbryyis1ceJEhYWFacOGDVqxYoW6deumnJwcdevWTbVr19bjjz+u2NhY7dq1S++//36ZPbjtttu0adMmXX755a7x3bt3a/369XrmmWdcY7NmzVKTJk3Up08fhYSEaOnSpbr//vvldDo1dOjQcvX+bFasWKGePXvqsssu0/jx4xUUFOQKUV9++aXatGkjSbr33nv17rvvatiwYWrcuLH++usvrVmzRj/++KNatWrllVqAKsUAUKWMHz/ekGQMGTLENXby5EmjTp06hs1mM6ZMmeIaP3TokBEREWEMHjzYNfaf//zHCAoKMr788ku39b788suGJGPt2rWusWrVqrkte2YNN91001nfK7Zjxw4jKCjI6Nevn1FUVOQ21+l0GoZhGIsWLTIkGZs2bfKsCf9fbm6uYbfbjYcffthtfNq0aYbNZjN2797tGisoKCixfPfu3Y0LL7zQbaxjx45Gx44dXa9nz55tSDJ27tzpNm/lypWGJGPlypWuz9KgQQOje/furs9VvN20tDSja9eurrGYmBhj6NCh5fqswPmMw0xAFfWPf/zD9ffg4GC1bt1ahmHorrvuco3Hxsbqkksu0a+//uoae+edd9SoUSM1bNhQBw4ccP3529/+JklauXKlxzXce++9Zc5ZvHixnE6nxo0bp6Ag9x9JxYejYmNjJUkffvihHA6Hx9uPjo5Wz549tXDhQrc9QwsWLFDbtm2VmprqGouIiHD9PTc3VwcOHFDHjh3166+/Kjc31+Ntns3WrVu1Y8cO3Xzzzfrrr79cfT169Ki6dOmi1atXy+l0Sjr1eTds2KB9+/ZVervA+YAwA1RRp39RS1JMTIzCw8NVq1atEuOHDh1yvd6xY4e+//571a5d2+3PxRdfLEnKycnxuIa0tLQy5/zyyy8KCgpS48aNzzqnY8eOuv766zVhwgTVqlVL1113nWbPnu12Ds/ZDBw4UHv37tW6detc29u8ebMGDhzoNm/t2rVKT09XtWrVFBsbq9q1a2v06NGS5JUws2PHDknS4MGDS/T23//+twoLC13bmTZtmr777julpKSoTZs2ysjIcAucANxxzgxQRQUHB3s0Jsltr4XT6VSzZs00Y8aMUuempKR4XMPpezsqw2az6d1339X69eu1dOlSffrpp7rzzjs1ffp0rV+/XtWrVz/rsr1791ZkZKQWLlyoK6+8UgsXLlRQUJAGDBjgmvPLL7+oS5cuatiwoWbMmKGUlBSFhYXp448/1rPPPuvaY3K22kpTVFTk9rp4Hc8884xatGhR6jLFn+OGG27Q1VdfrUWLFmn58uV65plnNHXqVL3//vvq2bPnWWsBzleEGQBu6tevr2+++UZdunQp80693riTb/369eV0OvXDDz+c9Uu+WNu2bdW2bVtNmjRJc+fO1aBBgzR//ny3Q2pnqlatmq699lq98847mjFjhhYsWKCrr75aycnJrjlLly5VYWGhlixZ4rZHy5NDajVq1JAkHT582G189+7dJT6ndOrQV3p6epnrTUpK0v3336/7779fOTk5atWqlSZNmkSYAUrBYSYAbm644Qb9/vvveu2110q8d+zYMdf9YqRTQeHML/Hy6tu3r4KCgjRx4sQSe0CK9xgdOnSoxNVQxcHH00NN+/bt07///W998803JQ4xFe+xOn0bubm5mj17dpnrLg4pq1evdo0VFRXp1VdfdZt32WWXqX79+vo//+f/KD8/v8R6/vzzT9eyZx7Wio+PV3JyskefFTgfsWcGgJtbb71VCxcu1L333quVK1eqffv2Kioq0k8//aSFCxfq008/dd2f5bLLLtNnn32mGTNmKDk5WWlpabriiivKtb2LLrpIY8aM0ZNPPqmrr75a/fv3l91u16ZNm5ScnKzJkyfrjTfe0EsvvaR+/fqpfv36OnLkiF577TVFR0erV69eZW6jV69eioqK0iOPPKLg4GBdf/31bu9369ZNYWFh6t27t+655x7l5+frtddeU3x8vPbv33/OdTdp0kRt27bVqFGjdPDgQcXFxWn+/Pk6efKk27ygoCD9+9//Vs+ePdWkSRPdcccduuCCC/T7779r5cqVio6O1tKlS3XkyBHVqVNHf//739W8eXNVr15dn332mTZt2lTqPYEAiEuzgaqm+NLnP//802188ODBRrVq1UrM79ixo9GkSRO3sRMnThhTp041mjRpYtjtdqNGjRrGZZddZkyYMMHIzc11zfvpp5+MDh06GBEREYYk12XaZ6vh9PfO9PrrrxstW7Z0ba9jx45GZmamYRiGsWXLFuOmm24yUlNTDbvdbsTHxxvXXnutkZWV5XFfBg0aZEgy0tPTS31/yZIlxqWXXmqEh4cb9erVM6ZOnWq8/vrrJS67PvPSbMMwjF9++cVIT0837Ha7kZCQYIwePdrIzMx0uzS72Ndff23079/fqFmzpmG32426desaN9xwg/H5558bhmEYhYWFxsiRI43mzZsbUVFRRrVq1YzmzZsbL730ksefFTjf2AzjHHeyAgAACHCcMwMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACytyt80z+l0at++fYqKivLKrdcBAIDvGYahI0eOKDk5WUFB5973UuXDzL59+8r1YDwAABA49u7dqzp16pxzTpUPM1FRUZJONSM6OrrC63E4HFq+fLm6deum0NBQb5WHUtBr89Br89Br89Br8/iy13l5eUpJSXF9j59LlQ8zxYeWoqOjKx1mIiMjFR0dzf85fIxem4dem4dem4dem8eMXntyiggnAAMAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEur8ncABgAg0BU5DW3ceVA5R44rPipcl9Wtoc27D7let0mLkyS3OaWNebKcN+ds3HlQmw/YVHPnQbW7KF7BQf55oDNhBgBQYWd+CQfiF2xF1mPm5zh09ISe/OgH7c897uprkE1yGv/rc2zkqUcFHC5wnHPMk+W8PydYb+7IUlJMuMb3bqweTZNkNsIMAJRDRX6DtuIXbEW/hAPtC7ai6zHzc5TmzPdPX/5cY54s56s52bnHdd9bWzTrllamBxrCDICAZeZv/b78DdqKX7AV/RIOtC/Yis4xc/tVhSHJJmnC0h/UtXGiqYecCDMAysWsY/u7DhRo3sY9ys4z57d+X/4GbcUv2PPpSxjeY0jan3tcG3ceVLv6NU3bLmHGAsw8MaxNWlyJNF3R3eoVrdEbu+PN/G3d33PMPLfA7GP7Z/J3UADgmZwjx8ue5EV+DTOrV6/WM888o82bN2v//v1atGiR+vbt6zbnxx9/1GOPPaYvvvhCJ0+eVOPGjfXee+8pNTXVP0X72JnBwewvj8Rou25qk6p6tap5abd6RWus3O54M39b9/ccM88tKI0v9ygAsKb4qHBTt+fXMHP06FE1b95cd955p/r371/i/V9++UVXXXWV7rrrLk2YMEHR0dH6/vvvFR5ubpN86fTwUtpu9dL48ssjO69Qz362o9Lbt+Iuc2pkzwSAyrFJSoz5395gs/g1zPTs2VM9e/Y86/tjxoxRr169NG3aNNdY/fr1zSjNFMu+268JS933egAAYEXFJyiM793Y9PvNBOw5M06nUx999JEeffRRde/eXV9//bXS0tI0atSoEoeiTldYWKjCwkLX67y8PEmSw+GQw1Hx3djFy1ZmHaf79Ps/9M/534hffAEAZyp5qDhEMmw6fMxxzjFPlvPVnMQYu8b0bKgul9TyyndledYRsGEmJydH+fn5mjJlip566ilNnTpVy5YtU//+/bVy5Up17Nix1OUmT56sCRMmlBhfvny5IiMjK11XZmZmhZd1GtIveTYdPiEt3hX0/4OMf+6WCKCiii9APddr+XCOL9dd+TmRIafGCk7+b8wmQ4bF5pi5/dgwQ33rOlU9VMpzSNGhUlqUoZ1HbK7X9aNPSjr1HXKuMU+W892coyravVkf75ZXFBQUeDzXZhhGQOwcsNlsbicA79u3TxdccIFuuukmzZ071zWvT58+qlatmubNm1fqekrbM5OSkqIDBw4oOjq6wvU5HA5lZmaqa9euCg0NLfF+kdNQ1u5DyjlSqPgou1rXrSFJrrHdfxVoQdZvys4rLLEsAOsy8zfhQPtNPCnGrlHdL1Fc9bCz/uyLj7KrZUqsvt572LQ5m3Ye0Ip1m/W3dpepbf3aFVqP2Z/DX48BqKyyvhsrIy8vT7Vq1VJubm6Z398Bu2emVq1aCgkJUePGjd3GGzVqpDVr1px1ObvdLrvdXmI8NDTUK40uXk9ZJ+56cpkpgPLx91VhSTHhGntNI9WoZi//ZfC//qnlX25Qt6uvqDJ3AC7tVg7Frro44ZyvfTmnfYN45e4w1L5BvOvnfkXW48saS5tjZd76jj1znZ4K2DATFhamyy+/XNu2bXMb3759u+rWreunqk7x5MRdQgzOF2beKiAQ7tdT2pd3aTcHO3PsirQ4/fWjoStOW8eZczxZT0WX8+UcwN/8Gmby8/P1888/u17v3LlTW7duVVxcnFJTUzVy5EgNHDhQHTp0UOfOnbVs2TItXbpUq1at8lvNgXDirpk3Kavo9rmHS9WssaJ7Jrx5E0fJd1/Uni4HILD4NcxkZWWpc+fOrtcjRoyQJA0ePFhz5sxRv3799PLLL2vy5MkaPny4LrnkEr333nu66qqr/FKv05Amf/yTqUHGzC+P0g6XVWa3eoXvAOyF3fGBcHddK9RYkV5XdM9EaWOeLgcA5xIwJwD7Sl5enmJiYjw6gehcHA6H/jXvE73wQ7AXqyuptN3qZp4YVtqD/cw+Mc3hcOjjjz9Wr169vH4MFu7otXnotXnotXl82evyfH8H7DkzgSjPR6fBxFUL1dhrmygx2j/h4XTBQTZ+MwYAWAphphyivRzwiyPL0/2aqUfTJO+uHACA80SQvwuwkvrRhhKj7V67zV1iTLhm3dKKIAMAQCWwZ6YcgmzSE70a6p/zv5FNOueJwJ5eZmrVGyUBABAoCDPl1L1Jgmbd0qrEfWY8uR8G4QUAAO8jzFRAj6ZJ6to4sUL3wwAAAN5FmKkgrvoBACAwcAIwAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNL+GmdWrV6t3795KTk6WzWbT4sWLzzr33nvvlc1m08yZM02rDwAABD6/hpmjR4+qefPmevHFF885b9GiRVq/fr2Sk5NNqgwAAFhFiD833rNnT/Xs2fOcc37//Xf985//1KeffqprrrnGpMoAAIBV+DXMlMXpdOrWW2/VyJEj1aRJE4+WKSwsVGFhoet1Xl6eJMnhcMjhcFS4luJlK7MOeIZem4dem4dem4dem8eXvS7POgM6zEydOlUhISEaPny4x8tMnjxZEyZMKDG+fPlyRUZGVrqmzMzMSq8DnqHX5qHX5qHX5qHX5vFFrwsKCjyeG7BhZvPmzfrXv/6lLVu2yGazebzcqFGjNGLECNfrvLw8paSkqFu3boqOjq5wPQ6HQ5mZmeratatCQ0MrvB6UjV6bh16bh16bh16bx5e9Lj6y4omADTNffvmlcnJylJqa6horKirSww8/rJkzZ2rXrl2lLme322W320uMh4aGeqXR3loPykavzUOvzUOvzUOvzeOLXpdnfQEbZm699Valp6e7jXXv3l233nqr7rjjDj9VBQAAAo1fw0x+fr5+/vln1+udO3dq69atiouLU2pqqmrWrOk2PzQ0VImJibrkkkvMLhUAAAQov4aZrKwsde7c2fW6+FyXwYMHa86cOX6qCgAAWIlfw0ynTp1kGIbH8892ngwAADh/8WwmAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaYQZAABgaX4NM6tXr1bv3r2VnJwsm82mxYsXu95zOBx67LHH1KxZM1WrVk3Jycm67bbbtG/fPv8VDAAAAo5fw8zRo0fVvHlzvfjiiyXeKygo0JYtWzR27Fht2bJF77//vrZt26Y+ffr4oVIAABCoQvy58Z49e6pnz56lvhcTE6PMzEy3sRdeeEFt2rTRnj17lJqaakaJAAAgwPk1zJRXbm6ubDabYmNjzzqnsLBQhYWFrtd5eXmSTh22cjgcFd528bKVWQc8Q6/NQ6/NQ6/NQ6/N48tel2edNsMwDK9XUAE2m02LFi1S3759S33/+PHjat++vRo2bKi33377rOvJyMjQhAkTSozPnTtXkZGR3ioXAAD4UEFBgW6++Wbl5uYqOjr6nHMtEWYcDoeuv/56/fbbb1q1atU5P1Rpe2ZSUlJ04MCBMptxLg6HQ5mZmeratatCQ0MrvB6UjV6bh16bh16bh16bx5e9zsvLU61atTwKMwF/mMnhcOiGG27Q7t27tWLFijI/kN1ul91uLzEeGhrqlUZ7az0oG702D702D702D702jy96XZ71BXSYKQ4yO3bs0MqVK1WzZk1/lwQAAAKMX8NMfn6+fv75Z9frnTt3auvWrYqLi1NSUpL+/ve/a8uWLfrwww9VVFSk7OxsSVJcXJzCwsL8VTYAAAggfg0zWVlZ6ty5s+v1iBEjJEmDBw9WRkaGlixZIklq0aKF23IrV65Up06dzCoTAAAEML+GmU6dOulc5x8HyLnJAAAggPFsJgAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGl+DTOrV69W7969lZycLJvNpsWLF7u9bxiGxo0bp6SkJEVERCg9PV07duzwT7EAACAg+TXMHD16VM2bN9eLL75Y6vvTpk3Tc889p5dfflkbNmxQtWrV1L17dx0/ftzkSgEAQKAK8efGe/bsqZ49e5b6nmEYmjlzpp544gldd911kqQ333xTCQkJWrx4sW688UYzSwUAAAEqYM+Z2blzp7Kzs5Wenu4ai4mJ0RVXXKF169b5sTIAABBI/Lpn5lyys7MlSQkJCW7jCQkJrvdKU1hYqMLCQtfrvLw8SZLD4ZDD4ahwPcXLVmYd8Ay9Ng+9Ng+9Ng+9No8ve12edQZsmKmoyZMna8KECSXGly9frsjIyEqvPzMzs9LrgGfotXnotXnotXnotXl80euCggKP5wZsmElMTJQk/fHHH0pKSnKN//HHH2rRosVZlxs1apRGjBjhep2Xl6eUlBR169ZN0dHRFa7H4XAoMzNTXbt2VWhoaIXXg7LRa/PQa/PQa/PQa/P4stfFR1Y8EbBhJi0tTYmJifr8889d4SUvL08bNmzQfffdd9bl7Ha77HZ7ifHQ0FCvNNpb60HZ6LV56LV56LV56LV5fNHr8qzPr2EmPz9fP//8s+v1zp07tXXrVsXFxSk1NVUPPvignnrqKTVo0EBpaWkaO3askpOT1bdvX/8VDQAAAopfw0xWVpY6d+7sel18eGjw4MGaM2eOHn30UR09elRDhgzR4cOHddVVV2nZsmUKDw/3V8kAACDA+DXMdOrUSYZhnPV9m82miRMnauLEiSZWBQAArCRg7zMDAADgCcIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwtIB9NhMAAFZQVFQkh8Ph7zL8wuFwKCQkRMePH1dRUVG5lg0NDVVwcLBX6iDMAABQAYZhKDs7W4cPH/Z3KX5jGIYSExO1d+9e2Wy2ci8fGxurxMTECi17OsIMAAAVUBxk4uPjFRkZWekvZCtyOp3Kz89X9erVFRTk+ZkrhmGooKBAOTk5kqSkpKRK1UGYAQCgnIqKilxBpmbNmv4ux2+cTqdOnDih8PDwcoUZSYqIiJAk5eTkKD4+vlKHnDgBGACAcio+RyYyMtLPlVhbcf8qe84RYQYAgAo6Hw8teZO3+keYAQAAllbhMPPzzz/r008/1bFjxySdOpkHAADAbOUOM3/99ZfS09N18cUXq1evXtq/f78k6a677tLDDz/s9QIBAKjKipyG1v3ylz7Y+rvW/fKXipzsHCivcl/N9NBDDykkJER79uxRo0aNXOMDBw7UiBEjNH36dK8WCABAVbXsu/2asPQH7c897hpLignX+N6N1aNp5S5XDhQOh0OhoaE+3Ua598wsX75cU6dOVZ06ddzGGzRooN27d3utMAAAqrJl3+3XfW9tcQsykpSde1z3vbVFy77b75PtdurUScOGDdOwYcMUExOjWrVqaezYsa7TRV566SU1aNBA4eHhSkhI0N///ne3ZYcPH65HH31UcXFxSk5O1pQpU9zWb7PZNGvWLPXp00fVqlXTpEmTfPI5TlfuPTNHjx4t9VK0gwcPym63e6UoAACsxjAMHXN4dkv/Iqeh8Uu+V2kHlAxJNkkZS35Q+4tqKTio7Ct+IkKDy3Vl0BtvvKG77rpLGzduVFZWloYMGaLU1FS1bNlSw4cP13/+8x9deeWVOnjwoL788ssSy44YMUIbNmzQ2rVrdeedd6pz587q3r27a05GRoamTJmimTNnKiTE97e0K/cWrr76ar355pt68sknJZ1KYE6nU9OmTVPnzp29XiAAAFZwzFGkxuM+9cq6DEnZecfVLGO5R/N/mNhdkWGef6WnpKTo2Weflc1m0yWXXKJvv/1Wzz77rJ566ilVq1ZN1157raKiolS3bl21bNnSbdlLL71U48ePlyTVr19fzz//vFasWOEWZm6++WbdcccdHtdTWeUOM9OmTVOXLl2UlZWlEydO6NFHH9X333+vgwcPau3atb6oEQAAeFHbtm3d9uS0a9dO06dPV5cuXVS3bl1deOGF6tGjh3r06KF+/fq5HZG59NJL3daVkJDgeixBsdatW/v2A5yh3GGmadOm2r59u1544QVFRUUpPz9f/fv319ChQyv9bAUAAKwqIjRYP0zsXvZESRt3HtTtszeVOW/OHZerTVqcR9v2hurVq2vLli1atWqVli9frnHjxikjI0ObNm1SbGysJJU4mbf4CM3pqlWr5pV6PFWhA1kxMTEaM2aMt2sBAMCybDabx4d6rm5QW0kx4crOPV7qeTM2SYkx4bq6QW2Pzpkprw0bNri9Xr9+vRo0aOB6PlJ6errS09M1fvx4xcbGasWKFerfv7/X6/CWcoeZ1atXn/P9Dh06VLgYAADOB8FBNo3v3Vj3vbVFNskt0BRHl/G9G/skyEjSnj17NGLECN1zzz3asmWLnn/+eU2fPl0ffvihfv31V3Xo0EE1atTQxx9/LKfTqUsuucQndXhLucNMp06dSoydftytqMizM7kBADif9WiapFm3tCpxn5lEE+4zc9ttt+nYsWNq06aNgoOD9cADD2jIkCFau3at3n//fWVkZOj48eNq0KCB5s2bpyZNmvisFm8od5g5dOiQ22uHw6Gvv/5aY8eONeVacgAAqooeTZPUtXGiNu48qJwjxxUfFa42aXE+2yNTLDQ0VDNnztSsWbPcxq+66iqtWrXqrMuV9t7bb7+t6Oho12t/PN6o3GEmJiamxFjXrl0VFhamESNGaPPmzV4pDACA80FwkE3t6tf0dxmW5rWnZickJGjbtm3eWh0AAIBHyr1n5r///a/ba8MwtH//fk2ZMkUtWrTwVl0AAMAHznUYyarKHWZatGghm81W4phY27Zt9frrr3utMAAAAE+UO8zs3LnT7XVQUJBq166t8PBwrxVVrKioSBkZGXrrrbeUnZ2t5ORk3X777XriiSfK9QwKAABQdZU7zNStW9cXdZRq6tSpmjVrlt544w01adJEWVlZuuOOOxQTE6Phw4ebVgcAAAhcHoWZ5557zuMVejNkfPXVV7ruuut0zTXXSJLq1aunefPmaePGjV7bBgAAsDaPwsyzzz7r0cpsNptXw8yVV16pV199Vdu3b9fFF1+sb775RmvWrNGMGTPOukxhYaEKCwtdr/Py8iSduh+Ow+GocC3Fy1ZmHfAMvTYPvTYPvTaPGb12OBwyDENOp7PEc4nOJ8Xnzxb3orycTqcMw5DD4XA9SqFYef79bIY/7m7jIafTqdGjR2vatGkKDg5WUVGRJk2apFGjRp11mYyMDE2YMKHE+Ny5c92e+gkAQEWFhIQoMTFRKSkpCgsL83c5lnXixAnt3btX2dnZOnnypNt7BQUFuvnmm5Wbm+t2U77SBHSYmT9/vkaOHKlnnnlGTZo00datW/Xggw9qxowZGjx4cKnLlLZnJiUlRQcOHCizGeficDiUmZmprl27lnhiKLyLXpuHXpuHXpvHjF4fP35ce/fuVb169XxyAUwgmjBhgj744ANt2bLFNWYYho4cOaKoqKgKXZhz/Phx7dq1SykpKSX6mJeXp1q1ankUZir01OzffvtNS5Ys0Z49e3TixAm39851CKi8Ro4cqccff1w33nijJKlZs2bavXu3Jk+efNYwY7fbZbfbS4yHhoZ65T9qb60HZaPX5qHX5qHX5vFlr4uKimSz2RQUFKSgoEref9ZZJO3+Ssr/Q6qeINW9UgoKLns5k40cOVLDhw93+7zFh5aKe1FeQUFBstlspf5bleffrtxh5vPPP1efPn104YUX6qefflLTpk21a9cuGYahVq1alXd151RQUFCiOcHBwef18UkAQBXywxJp2WNS3r7/jUUnSz2mSo37+K+uUlSvXl3Vq1f3dxmlKneMGjVqlB555BF9++23Cg8P13vvvae9e/eqY8eOGjBggFeL6927tyZNmqSPPvpIu3bt0qJFizRjxgz169fPq9sBAMB0PyyRFt7mHmQkKW//qfEflvhs0++++66aNWumiIgI1axZU+np6Tp69KhWrVqlNm3aqFq1aoqNjVX79u21e/duSafOST39Tv+33367+vXrp+nTpyspKUmxsbGaOHGiTp48qZEjRyouLk516tTR7NmzffY5ipV7z8yPP/6oefPmnVo4JETHjh1T9erVNXHiRF133XW67777vFbc888/r7Fjx+r+++9XTk6OkpOTdc8992jcuHFe2wYAAF5hGJKjwLO5ziLpk0cllXbaqiHJdmqPzYWdPDvkFBopeXjOyv79+3XTTTdp2rRp6tevn44cOaIvv/xShmGob9++uvvuuzVv3jydOHFCGzduPOe5MCtXrlR8fLxWrVqldevW6a677tJXX32lDh06aMOGDVqwYIHuuecede3aVXXq1PGovoood5ipVq2a6zyZpKQk/fLLL2rSpIkk6cCBA14tLioqSjNnztTMmTO9ul4AALzOUSA9neyllRmn9thMSfFs+uh9Ulg1j6bu379fJ0+eVP/+/V03wm3WrJkOHjyo3NxcXXvttapfv74kqVGjRudcV1xcnKZOnarY2Fg1atRI06ZNU0FBgUaPHi3p1NGcKVOmaM2aNa7zX32h3IeZ2rZtqzVr1kiSevXqpYcffliTJk3SnXfeqbZt23q9QAAA4D3NmzdXly5d1KxZMw0YMECvvfaaDh06pLi4ON1+++3q3r27evfurX/961/av3//OdfVuHFjt3NbExIS1KxZM9fr4OBg1axZUzk5OT77PFIF9szMmDFD+fn5kk5dppWfn68FCxaoQYMGXr2SCQAASwmNPLWHxBO7v5Le/nvZ8wa9e+rqJk+27aHg4GBlZmbqq6++0vLly/X8889rzJgx2rBhg2bPnq3hw4dr2bJlWrBggZ544gllZmaedWfFmVccFV+ZdOaYry/cKfeemaeffloHDx6UdOqQ08svv6z//ve/eu+990x9bhMAAAHFZjt1qMeTP/X/duqqJZ3tfBSbFH3BqXmerK+c93ix2Wxq3769JkyYoK+//lphYWFatGiRJKlly5YaNWqUvvrqKzVt2lRz586tXF9MUO4w8+eff6pHjx5KSUnRyJEj9c033/iiLgAAqq6g4FOXX0sqGWj+/+seU3xyv5kNGzbo6aefVlZWlvbs2aP3339ff/75pyIiIjRq1CitW7dOu3fv1vLly7Vjx44yz5sJBOUOMx988IH279+vsWPHatOmTWrVqpWaNGmip59+Wrt27fJBiQAAVEGN+0g3vClFJ7mPRyefGvfRfWaio6O1evVq9erVSxdffLGeeOIJTZ8+Xf3799dPP/2k66+/XhdffLGGDBmioUOH6p577vFJHd5UoTsA16hRQ0OGDNGQIUP022+/ad68eXr99dc1bty4Es9WAAAAZ9G4j9TwGlPvANyoUSMtW7as1PeKDzWVJiMjQxkZGa7Xc+bMkdPpdD3QWZJWrVpVYjkzdnRUKMwUczgcysrK0oYNG7Rr1y4lJCR4qy4AAM4PQcFS2tX+rsLSKvRAiZUrV+ruu+9WQkKCbr/9dkVHR+vDDz/Ub7/95u36AAAAzqnce2YuuOACHTx4UD169NCrr76q3r17l/pgRwAAADOUO8xkZGRowIABio2N9UE5AAAA5VPuMHP33Xf7og4AACzHMEp7thI85a3+VeicGQAAzmfFd7ktKPDwwZIoVXH/zrxrcHlV6momAADOR8HBwYqNjXU9cygyMvKcT5euqpxOp06cOKHjx4+7PaOpLIZhqKCgQDk5OYqNjVVwcOUuRSfMAABQAYmJiZLk84coBjLDMHTs2DFFRERUKMzFxsa6+lgZhBkAACrAZrMpKSlJ8fHxcjgc/i7HLxwOh1avXq0OHTqU+1BRaGhopffIFCPMAABQCcHBwV77Uraa4OBgnTx5UuHh4ZU+76UyOAEYAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYWsCHmd9//1233HKLatasqYiICDVr1kxZWVn+LgsAAASIEH8XcC6HDh1S+/bt1blzZ33yySeqXbu2duzYoRo1avi7NAAAECACOsxMnTpVKSkpmj17tmssLS3NjxUBAIBAE9BhZsmSJerevbsGDBigL774QhdccIHuv/9+3X333WddprCwUIWFha7XeXl5kiSHwyGHw1HhWoqXrcw64Bl6bR56bR56bR56bR5f9ro867QZhmF4vQIvCQ8PlySNGDFCAwYM0KZNm/TAAw/o5Zdf1uDBg0tdJiMjQxMmTCgxPnfuXEVGRvq0XgAA4B0FBQW6+eablZubq+jo6HPODegwExYWptatW+urr75yjQ0fPlybNm3SunXrSl2mtD0zKSkpOnDgQJnNOBeHw6HMzEx17dpVoaGhFV4PykavzUOvzUOvzUOvzePLXufl5alWrVoehZmAPsyUlJSkxo0bu401atRI77333lmXsdvtstvtJcZDQ0O90mhvrQdlo9fmodfmodfmodfm8UWvy7O+gL40u3379tq2bZvb2Pbt21W3bl0/VQQAAAJNQIeZhx56SOvXr9fTTz+tn3/+WXPnztWrr76qoUOH+rs0AAAQIAI6zFx++eVatGiR5s2bp6ZNm+rJJ5/UzJkzNWjQIH+XBgAAAkRAnzMjSddee62uvfZaf5cBAAACVEDvmQEAACgLYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYQYAAFiapcLMlClTZLPZ9OCDD/q7FAAAECAsE2Y2bdqkV155RZdeeqm/SwEAAAHEEmEmPz9fgwYN0muvvaYaNWr4uxwAABBAQvxdgCeGDh2qa665Runp6XrqqafOObewsFCFhYWu13l5eZIkh8Mhh8NR4RqKl63MOuAZem0eem0eem0eem0eX/a6POsM+DAzf/58bdmyRZs2bfJo/uTJkzVhwoQS48uXL1dkZGSl68nMzKz0OuAZem0eem0eem0eem0eX/S6oKDA47k2wzAMr1fgJXv37lXr1q2VmZnpOlemU6dOatGihWbOnFnqMqXtmUlJSdGBAwcUHR1d4VocDocyMzPVtWtXhYaGVng9KBu9Ng+9Ng+9Ng+9No8ve52Xl6datWopNze3zO/vgN4zs3nzZuXk5KhVq1ausaKiIq1evVovvPCCCgsLFRwc7LaM3W6X3W4vsa7Q0FCvNNpb60HZ6LV56LV56LV56LV5fNHr8qwvoMNMly5d9O2337qN3XHHHWrYsKEee+yxEkEGAACcfwI6zERFRalp06ZuY9WqVVPNmjVLjAMAgPOTJS7NBgAAOJuA3jNTmlWrVvm7BAAAEEDYMwMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACwt4MPM5MmTdfnllysqKkrx8fHq27evtm3b5u+yAABAgAj4MPPFF19o6NChWr9+vTIzM+VwONStWzcdPXrU36UBAIAAEOLvAsqybNkyt9dz5sxRfHy8Nm/erA4dOvipKgAAECgCPsycKTc3V5IUFxdX6vuFhYUqLCx0vc7Ly5MkORwOORyOCm+3eNnKrAOeodfmodfmodfmodfm8WWvy7NOm2EYhtcr8BGn06k+ffro8OHDWrNmTalzMjIyNGHChBLjc+fOVWRkpK9LBAAAXlBQUKCbb75Zubm5io6OPudcS4WZ++67T5988onWrFmjOnXqlDqntD0zKSkpOnDgQJnNOBeHw6HMzEx17dpVoaGhFV4PykavzUOvzUOvzUOvzePLXufl5alWrVoehRnLHGYaNmyYPvzwQ61evfqsQUaS7Ha77HZ7ifHQ0FCvNNpb60HZ6LV56LV56LV56LV5fNHr8qwv4MOMYRj65z//qUWLFmnVqlVKS0vzd0kAACCABHyYGTp0qObOnasPPvhAUVFRys7OliTFxMQoIiLCz9UBAAB/C/j7zMyaNUu5ubnq1KmTkpKSXH8WLFjg79IAAEAACPg9MxY6PxkAAPhBwO+ZAQAAOBfCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsDTCDAAAsLQQfxcAALAwZ5G0+ysp/w+peoJU90opKLjsOZL7WMoV0t4NXplj271WFxxcJ9vuaOnCDhVbj49r9Mocf2+/tF6f+W9vEsKMFZz5g8Ai/5FXtEav/CCqQj8sfFmjab32d48CoMYq+QVb8Jf06Sgpb59copOlbpOlajVPzfvrF2nLHPc5ETUk2aRjB/83ZguSDKdX5oQYTrWWpN2zKr4eH9folTn+3v6ZvY5OlnpMlRr3kdlshmEYpm/VRHl5eYqJiVFubq6io6MrvB6Hw6GPP/5YvXr1UmhoaMVWUpHfTkr7YWGR/8ipkRoDZvvU6Js5gBvbqf+54U2vBJryfH8TZjxUIsyUd29JRX87AQDAMmyn9tA8+G2lDzmV5/vbEoeZXnzxRT3zzDPKzs5W8+bN9fzzz6tNmzb+K+iHJdKyx8q/t+RMxw6VHCPIAAAsy5Dyfj/1i3za1aZtNeCvZlqwYIFGjBih8ePHa8uWLWrevLm6d++unJwcv9Rj++lDaeFt7kFGKhlCjh06d5ABAKCqyv/D1M0FfJiZMWOG7r77bt1xxx1q3LixXn75ZUVGRur11183vxjDqeDloyVV6SNzAABUTvUEUzcX0GHmxIkT2rx5s9LT011jQUFBSk9P17p160yvp2b+NtmO7Ct7IgAA5yWbFH3B/84bNUlAnzNz4MABFRUVKSHBPeElJCTop59+KnWZwsJCFRYWul7n5eVJOnUCr8PhqHAtDodD4Y7DFV4eAKoiQ65rWEp9fbZlVMZygTDH39u3Xo2nXhV1nSSjyCkVVe4c0PJ8Zwd0mKmIyZMna8KECSXGly9frsjIyEqtu2ZobKWWr6iyflhY4z9yaqRGaqyqNZ6prJ9ZJ4KrS5LsRfmnzbHJdtoa/T3H39u3Yo3HQmvouzqDtP/XIOnXj1VZBQUFHs8N6DBTq1YtBQcH648/3E8k+uOPP5SYmFjqMqNGjdKIESNcr/Py8pSSkqJu3bpV+tLszOVOOaOSZDuS7fYPWFEV/mFR4sqpuFMzT786qrQxT5Yzc46/t0+N1EiNlZsTfYGK0p+UIuNOuzfWQQVnPiGdfkg+KllFLW+VEXehVD1BQSntJEkn965zLWdc0EbO3ze6Xldmzond6/Tdus/UtF26gtOuqtB6fF2jN+b4e/tn9jo07Sq1DApWS3lH8ZEVTwT8fWauuOIKtWnTRs8//7wkyel0KjU1VcOGDdPjjz9e5vLevs/MNRc6FfLeHf9/tJKti4g79b/nus9M9AVSt6f/dzdNq94ptBw1nty1Vlu//FQtru6uEO4A7NMaTeu1v3sUADV6pdf+7lFpc0q7l4gnjzjwIa/c5BQe8WWvq9RN8xYsWKDBgwfrlVdeUZs2bTRz5kwtXLhQP/30U4lzaUrjk5vm7fjEg/vMlBJUopKly26Xatav/A+LKowfROah1+ah1+ah1+YJlDAT0IeZJGngwIH6888/NW7cOGVnZ6tFixZatmyZR0HGZxr3kRpeU7HfqkoLJmfeWMjEGw0BAGB1AR9mJGnYsGEaNmyYv8twFxTsWQghmAAA4FMBfZ8ZAACAshBmAACApRFmAACApRFmAACApRFmAACApRFmAACApRFmAACApRFmAACApRFmAACApVniDsCVUfzoqfI8fbM0DodDBQUFysvL41kfPkavzUOvzUOvzUOvzePLXhd/b3vyCMkqH2aOHDkiSUpJSfFzJQAAoLyOHDmimJiYc84J+KdmV5bT6dS+ffsUFRUlm81W4fXk5eUpJSVFe/furdTTt1E2em0eem0eem0eem0eX/baMAwdOXJEycnJCgo691kxVX7PTFBQkOrUqeO19UVHR/N/DpPQa/PQa/PQa/PQa/P4qtdl7ZEpxgnAAADA0ggzAADA0ggzHrLb7Ro/frzsdru/S6ny6LV56LV56LV56LV5AqXXVf4EYAAAULWxZwYAAFgaYQYAAFgaYQYAAFgaYQYAAFgaYcZDL774ourVq6fw8HBdccUV2rhxo79LsrTJkyfr8ssvV1RUlOLj49W3b19t27bNbc7x48c1dOhQ1axZU9WrV9f111+vP/74w08VVx1TpkyRzWbTgw8+6Bqj197z+++/65ZbblHNmjUVERGhZs2aKSsry/W+YRgaN26ckpKSFBERofT0dO3YscOPFVtXUVGRxo4dq7S0NEVERKh+/fp68skn3Z7lQ78rZvXq1erdu7eSk5Nls9m0ePFit/c96evBgwc1aNAgRUdHKzY2VnfddZfy8/N9U7CBMs2fP98ICwszXn/9deP777837r77biM2Ntb4448//F2aZXXv3t2YPXu28d133xlbt241evXqZaSmphr5+fmuOffee6+RkpJifP7550ZWVpbRtm1b48orr/Rj1da3ceNGo169esall15qPPDAA65xeu0dBw8eNOrWrWvcfvvtxoYNG4xff/3V+PTTT42ff/7ZNWfKlClGTEyMsXjxYuObb74x+vTpY6SlpRnHjh3zY+XWNGnSJKNmzZrGhx9+aOzcudN45513jOrVqxv/+te/XHPod8V8/PHHxpgxY4z333/fkGQsWrTI7X1P+tqjRw+jefPmxvr1640vv/zSuOiii4ybbrrJJ/USZjzQpk0bY+jQoa7XRUVFRnJysjF58mQ/VlW15OTkGJKML774wjAMwzh8+LARGhpqvPPOO645P/74oyHJWLdunb/KtLQjR44YDRo0MDIzM42OHTu6wgy99p7HHnvMuOqqq876vtPpNBITE41nnnnGNXb48GHDbrcb8+bNM6PEKuWaa64x7rzzTrex/v37G4MGDTIMg357y5lhxpO+/vDDD4YkY9OmTa45n3zyiWGz2Yzff//d6zVymKkMJ06c0ObNm5Wenu4aCwoKUnp6utatW+fHyqqW3NxcSVJcXJwkafPmzXI4HG59b9iwoVJTU+l7BQ0dOlTXXHONW08leu1NS5YsUevWrTVgwADFx8erZcuWeu2111zv79y5U9nZ2W69jomJ0RVXXEGvK+DKK6/U559/ru3bt0uSvvnmG61Zs0Y9e/aURL99xZO+rlu3TrGxsWrdurVrTnp6uoKCgrRhwwav11TlHzRZWQcOHFBRUZESEhLcxhMSEvTTTz/5qaqqxel06sEHH1T79u3VtGlTSVJ2drbCwsIUGxvrNjchIUHZ2dl+qNLa5s+fry1btmjTpk0l3qPX3vPrr79q1qxZGjFihEaPHq1NmzZp+PDhCgsL0+DBg139LO3nCb0uv8cff1x5eXlq2LChgoODVVRUpEmTJmnQoEGSRL99xJO+ZmdnKz4+3u39kJAQxcXF+aT3hBn43dChQ/Xdd99pzZo1/i6lStq7d68eeOABZWZmKjw83N/lVGlOp1OtW7fW008/LUlq2bKlvvvuO7388ssaPHiwn6urehYuXKi3335bc+fOVZMmTbR161Y9+OCDSk5Opt/nGQ4zlaFWrVoKDg4ucWXHH3/8ocTERD9VVXUMGzZMH374oVauXKk6deq4xhMTE3XixAkdPnzYbT59L7/NmzcrJydHrVq1UkhIiEJCQvTFF1/oueeeU0hIiBISEui1lyQlJalx48ZuY40aNdKePXskydVPfp54x8iRI/X444/rxhtvVLNmzXTrrbfqoYce0uTJkyXRb1/xpK+JiYnKyclxe//kyZM6ePCgT3pPmClDWFiYLrvsMn3++eeuMafTqc8//1zt2rXzY2XWZhiGhg0bpkWLFmnFihVKS0tze/+yyy5TaGioW9+3bdumPXv20Pdy6tKli7799ltt3brV9ad169YaNGiQ6+/02jvat29f4hYD27dvV926dSVJaWlpSkxMdOt1Xl6eNmzYQK8roKCgQEFB7l9jwcHBcjqdkui3r3jS13bt2unw4cPavHmza86KFSvkdDp1xRVXeL8or59SXAXNnz/fsNvtxpw5c4wffvjBGDJkiBEbG2tkZ2f7uzTLuu+++4yYmBhj1apVxv79+11/CgoKXHPuvfdeIzU11VixYoWRlZVltGvXzmjXrp0fq646Tr+ayTDotbds3LjRCAkJMSZNmmTs2LHDePvtt43IyEjjrbfecs2ZMmWKERsba3zwwQfGf//7X+O6667jUuEKGjx4sHHBBRe4Ls1+//33jVq1ahmPPvqoaw79rpgjR44YX3/9tfH1118bkowZM2YYX3/9tbF7927DMDzra48ePYyWLVsaGzZsMNasWWM0aNCAS7P97fnnnzdSU1ONsLAwo02bNsb69ev9XZKlSSr1z+zZs11zjh07Ztx///1GjRo1jMjISKNfv37G/v37/Vd0FXJmmKHX3rN06VKjadOmht1uNxo2bGi8+uqrbu87nU5j7NixRkJCgmG3240uXboY27Zt81O11paXl2c88MADRmpqqhEeHm5ceOGFxpgxY4zCwkLXHPpdMStXriz1Z/TgwYMNw/Csr3/99Zdx0003GdWrVzeio6ONO+64wzhy5IhP6rUZxmm3SgQAALAYzpkBAACWRpgBAACWRpgBAACWRpgBAACWRpgBAACWRpgBAACWRpgBAACWRpgBcN5ZtWqVbDZbiedRAbAmwgwAALA0wgwAALA0wgwA0zmdTk2ePFlpaWmKiIhQ8+bN9e6770r63yGgjz76SJdeeqnCw8PVtm1bfffdd27reO+999SkSRPZ7XbVq1dP06dPd3u/sLBQjz32mFJSUmS323XRRRfp//7f/+s2Z/PmzWrdurUiIyN15ZVXlnjiNQBrIMwAMN3kyZP15ptv6uWXX9b333+vhx56SLfccou++OIL15yRI0dq+vTp2rRpk2rXrq3evXvL4XBIOhVCbrjhBt1444369ttvlZGRobFjx2rOnDmu5W+77TbNmzdPzz33nH788Ue98sorql69ulsdY8aM0fTp05WVlaWQkBDdeeedpnx+AN7FgyYBmKqwsFBxcXH67LPP1K5dO9f4P/7xDxUUFGjIkCHq3Lmz5s+fr4EDB0qSDh48qDp16mjOnDm64YYbNGjQIP35559avny5a/lHH31UH330kb7//ntt375dl1xyiTIzM5Wenl6ihlWrVqlz58767LPP1KVLF0nSxx9/rGuuuUbHjh1TeHi4j7sAwJvYMwPAVD///LMKCgrUtWtXVa9e3fXnzTff1C+//OKad3rQiYuL0yWXXKIff/xRkvTjjz+qffv2butt3769duzYoaKiIm3dulXBwcHq2LHjOWu59NJLXX9PSkqSJOXk5FT6MwIwV4i/CwBwfsnPz5ckffTRR7rgggvc3rPb7W6BpqIiIiI8mhcaGur6u81mk3TqfB4A1sKeGQCmaty4sex2u/bs2aOLLrrI7U9KSopr3vr1611/P3TokLZv365GjRpJkho1aqS1a9e6rXft2rW6+OKLFRwcrGbNmsnpdLqdgwOg6mLPDABTRUVF6ZFHHtFDDz0kp9Opq666Srm5uVq7dq2io6NVt25dSdLEiRNVs2ZNJSQkaMyYMapVq5b69u0rSXr44Yd1+eWX68knn9TAgQO1bt06vfDCC3rppZckSfXq1dPgwYN155136rnnnlPz5s21e/du5eTk6IYbbvDXRwfgI4QZAKZ78sknVbt2bU2ePFm//vqrYmNj1apVK40ePdp1mGfKlCl64IEHtGPHDrVo0UJLly5VWFiYJKlVq1ZauHChxo0bpyeffFJJSUmaOHGibr/9dtc2Zs2apdGjR+v+++/XX3/9pdTUVI0ePdofHxeAj3E1E4CAUnyl0aFDhxQbG+vvcgBYAOfMAAAASyPMAAAAS+MwEwAAsDT2zAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEv7f39vWqg5EIXsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# epoch_psnr = []\n",
    "# epoch_ssim = []\n",
    "\n",
    "# epoch_exp_psnr = []\n",
    "# epoch_exp_ssim = []\n",
    "\n",
    "epoch_val_psnr = []\n",
    "epoch_val_ssim = []\n",
    "\n",
    "for epoch in range(epoch_init, opt.n_epoch + 1):\n",
    "    cnt = 0\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        current_lr = param_group['lr']\n",
    "    print(\"LearningRate of Epoch {} = {}\".format(epoch, current_lr))\n",
    "\n",
    "    network.train()\n",
    "\n",
    "    # img_psnr = []\n",
    "    # img_ssim = []\n",
    "\n",
    "    # img_exp_psnr = []\n",
    "    # img_exp_ssim = []\n",
    "    for noisy, name in tqdm(train_dataloader):\n",
    "\n",
    "        st = time.time()\n",
    "        # noisy = noisy / 255.0\n",
    "        # print(noisy.shape)\n",
    "\n",
    "        noisy = noisy.cuda()\n",
    "        # print('noisy_mean', noisy.mean())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        net_input, mask = masker.train(noisy)\n",
    "        noisy_output = network(net_input)\n",
    "        n, c, h, w = noisy.shape\n",
    "        noisy_output = (noisy_output*mask).view(n, -1, c, h, w).sum(dim=1)\n",
    "        diff = noisy_output - noisy\n",
    "\n",
    "        # psnr and ssim for one image\n",
    "        # target = torch.unsqueeze(noisy, 0).permute(0, 2, 3, 1).cpu().data.clamp(0, 1).numpy().squeeze(0).astype(np.float32)\n",
    "        # ref = diff.permute(0, 2, 3, 1).cpu().data.clamp(0, 1).numpy().squeeze(0).astype(np.float32)\n",
    "        # img_psnr.append(calculate_psnr(target, ref))\n",
    "        # img_ssim.append(calculate_ssim(target, ref))\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            exp_output = network(noisy)\n",
    "        exp_diff = exp_output - noisy\n",
    "\n",
    "        # psnr and ssim for exp\n",
    "        # ref_exp = exp_diff.permute(0, 2, 3, 1).cpu().data.clamp(0, 1).numpy().squeeze(0).astype(np.float32)\n",
    "        # img_exp_psnr.append(calculate_psnr(target, ref_exp))\n",
    "        # img_exp_ssim.append(calculate_ssim(target, ref_exp))\n",
    "\n",
    "        # g25, p30: 1_1-2; frange-10\n",
    "        # g5-50 | p5-50 | raw; 1_1-2; range-10\n",
    "        Lambda = epoch / opt.n_epoch\n",
    "        if Lambda <= Thread1:\n",
    "            beta = Lambda2\n",
    "        elif Thread1 <= Lambda <= Thread2:\n",
    "            beta = Lambda2 + (Lambda - Thread1) * \\\n",
    "                (increase_ratio-Lambda2) / (Thread2-Thread1)\n",
    "        else:\n",
    "            beta = increase_ratio\n",
    "        alpha = Lambda1\n",
    "\n",
    "        revisible = diff + beta * exp_diff\n",
    "        loss_reg = alpha * torch.mean(diff**2)\n",
    "        loss_rev = torch.mean(revisible**2)\n",
    "        loss_all = loss_reg + loss_rev\n",
    "\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # epoch_psnr.append(np.mean(img_psnr))\n",
    "    # epoch_ssim.append(np.mean(img_ssim))\n",
    "\n",
    "    # epoch_exp_psnr.append(np.mean(img_exp_psnr))\n",
    "    # epoch_exp_ssim.append(np.mean(img_exp_ssim))\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % opt.n_snapshot == 0 or epoch == opt.n_epoch:\n",
    "        network.eval()\n",
    "        # save checkpoint\n",
    "        save_network(network, epoch, \"model\", opt=opt)\n",
    "        save_state(epoch, optimizer, scheduler, opt=opt)\n",
    "\n",
    "        # validation\n",
    "        np.random.seed(101)\n",
    "\n",
    "        save_dir = os.path.join(validation_path, f'epoch_{epoch}')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        # for idx in range(num_img):\n",
    "        for im, name in test_dataloader:\n",
    "            img_avg_psnr_mid = []\n",
    "            img_avg_ssim_mid = []\n",
    "            \n",
    "            if type(im) == torch.Tensor:\n",
    "                if len(im.shape) == 4: \n",
    "                    im = im.squeeze(0)\n",
    "                im = im.permute(1,2,0)\n",
    "                im = im.numpy()\n",
    "            origin255 = im.copy() * 255\n",
    "            origin255 = origin255.astype(np.uint8)\n",
    "            im = np.array(im, dtype=np.float32) #/ 255.0\n",
    "            noisy_im = im.copy() # add\n",
    "            noisy255 = noisy_im.copy()\n",
    "            noisy255 = np.clip(noisy255 * 255.0 + 0.5, 0,\n",
    "                                255).astype(np.uint8)\n",
    "\n",
    "            # padding to square\n",
    "            H = noisy_im.shape[0]\n",
    "            W = noisy_im.shape[1]\n",
    "            val_size = (max(H, W) + 31) // 32 * 32\n",
    "            noisy_im = np.pad(\n",
    "                noisy_im,\n",
    "                [[0, val_size - H], [0, val_size - W], [0, 0]],\n",
    "                'reflect')\n",
    "            transformer = transforms.Compose([transforms.ToTensor()])\n",
    "            noisy_im = transformer(noisy_im)\n",
    "            noisy_im = torch.unsqueeze(noisy_im, 0)\n",
    "            noisy_im = noisy_im.cuda()\n",
    "            with torch.no_grad():\n",
    "                n, c, h, w = noisy_im.shape\n",
    "                net_input, mask = masker.train(noisy_im)\n",
    "                noisy_output = (network(net_input) *\n",
    "                                mask).view(n, -1, c, h, w).sum(dim=1)\n",
    "                dn_output = noisy_output.detach().clone()\n",
    "                # Release gpu memory\n",
    "                del net_input, mask, noisy_output\n",
    "                torch.cuda.empty_cache()\n",
    "                exp_output = network(noisy_im)\n",
    "            pred_dn = dn_output[:, :, :H, :W]\n",
    "            pred_exp = exp_output.detach().clone()[:, :, :H, :W]\n",
    "            # print('pred_exp mean', pred_exp.mean())\n",
    "            pred_mid = (pred_dn + beta*pred_exp) / (1 + beta)\n",
    "\n",
    "            # Release gpu memory\n",
    "            del exp_output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            pred_dn = pred_dn.permute(0, 2, 3, 1)\n",
    "            pred_exp = pred_exp.permute(0, 2, 3, 1)\n",
    "            pred_mid = pred_mid.permute(0, 2, 3, 1)\n",
    "\n",
    "            pred_dn = pred_dn.cpu().data.clamp(0, 1).numpy().squeeze(0)\n",
    "            pred_exp = pred_exp.cpu().data.clamp(0, 1).numpy().squeeze(0)\n",
    "            pred_mid = pred_mid.cpu().data.clamp(0, 1).numpy().squeeze(0)\n",
    "\n",
    "            pred255_dn = np.clip(pred_dn * 255.0 + 0.5, 0,\n",
    "                                    255).astype(np.uint8)\n",
    "            pred255_exp = np.clip(pred_exp * 255.0 + 0.5, 0,\n",
    "                                    255).astype(np.uint8)\n",
    "            pred255_mid = np.clip(pred_mid * 255.0 + 0.5, 0,\n",
    "                                    255).astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "            # calculate psnr and ssim for val dataset\n",
    "            psnr_mid = calculate_psnr(origin255.astype(np.float32),\n",
    "                                        pred255_mid.astype(np.float32))\n",
    "            img_avg_psnr_mid.append(psnr_mid)\n",
    "            ssim_mid = calculate_ssim(origin255.astype(np.float32),\n",
    "                                        pred255_mid.astype(np.float32))\n",
    "            img_avg_ssim_mid.append(ssim_mid)\n",
    "\n",
    "\n",
    "            # visualization\n",
    "            if opt.n_channel == 1:\n",
    "                color_mode = 'L'\n",
    "                save_path = os.path.join(save_dir, f\"{name}_clean.png\")\n",
    "                Image.fromarray(origin255.squeeze()).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_noisy.png\")    \n",
    "                Image.fromarray(noisy255.squeeze()).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_dn.png\") \n",
    "                Image.fromarray(pred255_dn.squeeze()).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_exp.png\") \n",
    "                Image.fromarray(pred255_exp.squeeze()).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_mid.png\") \n",
    "                Image.fromarray(pred255_mid.squeeze()).convert(color_mode).save(save_path)\n",
    "            else:\n",
    "                color_mode = 'L'\n",
    "                save_path = os.path.join(save_dir, f\"{name}_clean.png\")\n",
    "                Image.fromarray(origin255).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_noisy.png\")    \n",
    "                Image.fromarray(noisy255).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_dn.png\") \n",
    "                Image.fromarray(pred255_dn).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}_exp.png\") \n",
    "                Image.fromarray(pred255_exp).convert(color_mode).save(save_path)\n",
    "\n",
    "                save_path = os.path.join(save_dir, f\"{name}-_mid.png\") \n",
    "                Image.fromarray(pred255_mid).convert(color_mode).save(save_path)\n",
    "\n",
    "        epoch_val_psnr.append(np.mean(img_avg_psnr_mid))\n",
    "        epoch_val_ssim.append(np.mean(img_avg_ssim_mid))\n",
    "            \n",
    "\n",
    "# psnr = np.mean(epoch_psnr)\n",
    "# ssim = np.mean(epoch_ssim)\n",
    "# psnr_exp = np.mean(epoch_exp_psnr)\n",
    "# ssim_exp = np.mean(epoch_exp_ssim)\n",
    "\n",
    "# print(f'average psnr: {psnr}\\n average ssim: {ssim}\\naverage exp psnr: {exp_psnr}\\naverage exp ssim: {exp_ssim}\\n')\n",
    "    \n",
    "print(f'average psnr for val: {np.mean(epoch_val_psnr)}\\naverage ssim for val: {np.mean(epoch_val_ssim)}')\n",
    "write_in_file(epoch_val_ssim, 'ssim', opt.train_dataset_name)\n",
    "write_in_file(epoch_val_psnr, 'psnr', opt.train_dataset_name)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(1, len(epoch_val_psnr) + 1), epoch_val_psnr, marker='o', linestyle='-', label='psnr')\n",
    "plt.plot(np.arange(1, len(epoch_val_ssim) + 1), epoch_val_ssim, marker='o', linestyle='-', label='ssim')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('value')\n",
    "plt.title('metrics values')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(f'./figs/psnr and ssim for {opt.train_dataset_name}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--noisetype\", type=str, default=\"gauss25\", choices=['gauss25', 'gauss5_50', 'poisson30', 'poisson5_50'])\n",
    "parser.add_argument('--checkpoint', type=str, default='./*.pth')\n",
    "parser.add_argument('--test_dir', type=str, default='./data/test')\n",
    "parser.add_argument('--save_test_path', type=str, default='./test')\n",
    "parser.add_argument('--log_name', type=str, default='b2u_unet_g25_112rf20')\n",
    "parser.add_argument('--gpu_devices', default='0', type=str)\n",
    "parser.add_argument('--parallel', action='store_true')\n",
    "parser.add_argument('--n_feature', type=int, default=48)\n",
    "parser.add_argument('--n_channel', type=int, default=1)\n",
    "parser.add_argument(\"--beta\", type=float, default=20.0)\n",
    "parser.add_argument('--dataset_name', type=str)\n",
    "parser.add_argument('--repeat_times', type=int, default=1)\n",
    "parser.add_argument('--_continue', type=bool, default=True)\n",
    "\n",
    "opt = parser.parse_args(['--noisetype', 'gauss5_50', \n",
    "                        # '--checkpoint', './pretrained_models/g5-50_112rf20_beta19.4.pth',\n",
    "                        # '--checkpoint', './experiments/my_models/b2u_second/models/epoch_model_100.pth',\n",
    "                        '--checkpoint', './experiments/my_models/b2u_g146_first/models/epoch_model_100.pth',\n",
    "                        '--test_dir', './data/train',\n",
    "                        '--save_test_path', './test',\n",
    "                        '--log_name', 'b2u_g146_first',\n",
    "                        '--beta', '19.4',\n",
    "                        '--dataset_name', 'G146_first'])\n",
    "                        \n",
    "systime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "operation_seed_counter = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_devices\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_cropping():\n",
    "    def __init__(self, img_size, crop_size, margin) -> None:\n",
    "        self.margin = margin\n",
    "        self.img_size = img_size\n",
    "        self.stride_v = crop_size[0] - margin\n",
    "        self.stride_h = crop_size[1] - margin\n",
    "        self.crop_size = crop_size\n",
    "        self.h_steps = int(np.ceil(img_size[1]/self.stride_h))\n",
    "        self.v_steps = int(np.ceil(img_size[0]/self.stride_v))\n",
    "        self.crop_nums = self.h_steps * self.v_steps\n",
    "\n",
    "    def get_position(self, index):\n",
    "        v_step = int(np.floor(index/self.v_steps))\n",
    "        h_step = index - (v_step*self.v_steps)\n",
    "\n",
    "        return v_step, h_step\n",
    "    \n",
    "    \n",
    "    def crop_image(self, img, index):\n",
    "        v_step, h_step = self.get_position(index)\n",
    "\n",
    "        return img[v_step*self.stride_v : v_step*self.stride_v + self.crop_size[0],\n",
    "                                     h_step*self.stride_h : h_step*self.stride_h + self.crop_size[1]]\n",
    "    \n",
    "    def concat(self, img_array):\n",
    "        img = np.empty(self.img_size)\n",
    "        i = 0\n",
    "        for v in range(self.v_steps):\n",
    "            for h in range(self.h_steps):\n",
    "                flag_v = 1\n",
    "                flag_h = 1\n",
    "\n",
    "                if v == 0:\n",
    "                    flag_v = 0\n",
    "                if h == 0:\n",
    "                    flag_h = 0\n",
    "\n",
    "                \n",
    "                img[v*self.stride_v + flag_v * (self.margin//2): v*self.stride_v + self.crop_size[0],\n",
    "                                     h*self.stride_h + flag_h * (self.margin//2):h*self.stride_h + self.crop_size[1]] = img_array[i] [flag_v * (self.margin//2):, flag_h * (self.margin//2):]\n",
    "\n",
    "\n",
    "                i+=1\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_median(img, percentile):\n",
    "    tresh = np.percentile(img.ravel(), percentile)\n",
    "    mask = np.zeros_like(img)\n",
    "    mask[img < tresh] = 1\n",
    "    median_img = cv2.medianBlur(img, 7)\n",
    "    img = img*(1 - mask) + median_img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroscopyDataset(Dataset):\n",
    "    def __init__(self, img_dir, cropping, channels=3, transform=None, apply_median=False):\n",
    "        self.cropping_object = cropping\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.crop_nums = cropping.crop_nums\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "        self.apply_median = apply_median\n",
    "        if channels == 1:\n",
    "            self.channels = 0\n",
    "        else:\n",
    "            self.channels = channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.img_filenames) * self.crop_nums)\n",
    "\n",
    "    def __getitem__(self, crop_idx):\n",
    "        image_idx = crop_idx // self.crop_nums\n",
    "        img_crop_idx = crop_idx % self.crop_nums\n",
    "        # print(f'img index {image_idx}, img_crop_idx {img_crop_idx}')\n",
    "        name = os.path.split(self.img_filenames[image_idx])[-1].split('.')[0]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.img_filenames[image_idx]), self.channels)\n",
    "\n",
    "        crop = self.cropping_object.crop_image(image, img_crop_idx)\n",
    "\n",
    "        if self.apply_median:\n",
    "            crop = apply_median(crop, 10)\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        return crop, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all images:  321\n",
      "prepared images:  0\n",
      "rest images:  321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (head): Sequential(\n",
       "    (0): LR(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): LR(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_path): ModuleList(\n",
       "    (0-4): 5 x LR(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_path): ModuleList(\n",
       "    (0): UP(\n",
       "      (conv_1): LR(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv_2): LR(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1-3): 3 x UP(\n",
       "      (conv_1): LR(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(144, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv_2): LR(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): UP(\n",
       "      (conv_1): LR(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(97, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv_2): LR(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (last): Sequential(\n",
       "    (0): LR(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): LR(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation\n",
    "save_test_path = os.path.join(opt.save_test_path, opt.dataset_name)\n",
    "os.makedirs(save_test_path, exist_ok=True)\n",
    "\n",
    "validation_path = os.path.join(save_test_path, opt.log_name)\n",
    "os.makedirs(validation_path, exist_ok=True)\n",
    "np.random.seed(101)\n",
    "\n",
    "\n",
    "# Dataset\n",
    "test_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "dataset_dir = os.path.join(opt.test_dir, opt.dataset_name)\n",
    "cropping = Image_cropping((4096, 6144), (768, 1024), 200)\n",
    "test_dataset = MicroscopyDataset(dataset_dir, cropping, opt.n_channel, transform=test_transforms, apply_median=True)\n",
    "if opt._continue == True:\n",
    "    ready_filenames = os.listdir(validation_path)\n",
    "    ready_filenames = [x.split('.')[0] for x in ready_filenames]\n",
    "    filenames = os.listdir(dataset_dir)\n",
    "    print('all images: ', len(filenames))\n",
    "    print('prepared images: ', len(ready_filenames))\n",
    "\n",
    "    rest_filenames = [x for x in filenames if x.split('.')[0] not in ready_filenames]\n",
    "    print('rest images: ', len(rest_filenames))\n",
    "    test_dataset.img_filenames = rest_filenames\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Masker\n",
    "masker = Masker(width=2, mode='interpolate', mask_type='all')\n",
    "# Network\n",
    "network = UNet(in_channels=opt.n_channel,\n",
    "                out_channels=opt.n_channel,\n",
    "                wf=opt.n_feature)\n",
    "if opt.parallel:\n",
    "    network = torch.nn.DataParallel(network)\n",
    "network = network.cuda()\n",
    "# load pre-trained model\n",
    "network = load_network(opt.checkpoint, network, strict=True)\n",
    "beta = opt.beta\n",
    "\n",
    "# turn on eval mode\n",
    "network.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20544/20544 [2:52:53<00:00,  1.98it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_dir = validation_path\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "repeat_times = opt.repeat_times\n",
    "\n",
    "for i in range(repeat_times):\n",
    "    mid = []\n",
    "    for im, name in tqdm(test_dataloader):\n",
    "        # print('im shape', im.shape)\n",
    "        \n",
    "        if type(im) == torch.Tensor:\n",
    "            \n",
    "            if len(im.shape) == 4: \n",
    "                im = im.squeeze(0)\n",
    "            im = im.permute(1,2,0)\n",
    "            im = im.numpy()\n",
    "        # print(im.shape)\n",
    "        origin255 = im.copy() * 255\n",
    "        origin255 = origin255.astype(np.uint8)\n",
    "        im = np.array(im, dtype=np.float32) #/ 255.0\n",
    "        # noisy_im = noise_adder.add_valid_noise(im)\n",
    "        \n",
    "        noisy_im = im.copy() # add\n",
    "        # print('noisy_im.shape ', noisy_im.shape)\n",
    "        noisy255 = noisy_im.copy()\n",
    "        noisy255 = np.clip(noisy255 * 255.0 + 0.5, 0,\n",
    "                            255).astype(np.uint8)\n",
    "\n",
    "\n",
    "        # padding to square\n",
    "        H = noisy_im.shape[0]\n",
    "        W = noisy_im.shape[1]\n",
    "        val_size = (max(H, W) + 31) // 32 * 32\n",
    "        noisy_im = np.pad(\n",
    "            noisy_im,\n",
    "            [[0, val_size - H], [0, val_size - W], [0, 0]],\n",
    "            'reflect')\n",
    "\n",
    "        transformer = transforms.Compose([transforms.ToTensor()])\n",
    "        noisy_im = transformer(noisy_im)\n",
    "        noisy_im = torch.unsqueeze(noisy_im, 0)\n",
    "        noisy_im = noisy_im.cuda()\n",
    "        with torch.no_grad():\n",
    "            n, c, h, w = noisy_im.shape\n",
    "            # print('noisy_im.shape ', noisy_im.shape)\n",
    "            net_input, mask = masker.train(noisy_im)\n",
    "            # print('net_input shape', net_input.shape)\n",
    "            noisy_output = (network(net_input)*mask).view(n,-1,c,h,w).sum(dim=1)\n",
    "            exp_output = network(noisy_im)\n",
    "        pred_dn = noisy_output[:, :, :H, :W]\n",
    "        pred_exp = exp_output[:, :, :H, :W]\n",
    "        pred_mid = (pred_dn + beta*pred_exp) / (1 + beta)\n",
    "\n",
    "        pred_mid = pred_mid.permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "        pred_mid = pred_mid.cpu().data.clamp(0, 1).numpy().squeeze(0)\n",
    "        pred255_mid = np.clip(pred_mid * 255.0 + 0.5, 0,\n",
    "                            255).astype(np.uint8)                   \n",
    "\n",
    "\n",
    "        mid.append(pred255_mid.squeeze())\n",
    "\n",
    "        if len(mid) == cropping.crop_nums:\n",
    "            mid_img = cropping.concat(mid)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        color_mode = 'L'\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"{name[0]}.png\") \n",
    "        Image.fromarray(mid_img).convert(color_mode).save(save_path)\n",
    "\n",
    "        mid = []\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('b2u')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f6574e0f1b25f170aaba03e2351043641f02eb46efc17834ea2ed04c3a8ee1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
